{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9829c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones básicas e instalación de recursos NLTK\n",
    "\n",
    "\n",
    "import os, re, json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "\n",
    "# Descarga simple de recursos NLTK si faltan\n",
    "try:\n",
    "    _ = stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "try:\n",
    "    _ = wn.synsets(\"good\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ee7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion\n",
    "\n",
    "DATA_PATH = \"dataset\\df_200k.csv\"  # ruta al CSV\n",
    "TEXT_COL  = \"text\"                               # nombre de la columna de texto\n",
    "ID_COL    = None                                 # usa None si no tienes id\n",
    "LANG      = \"en\"                                 # \"en\" para inglés, \"es\" para español\n",
    "USE_WORDNET = True                               # activa/desactiva expansión con WordNet\n",
    "TAU_PERCENTILE = 35                               # umbral de neutralidad (percentil)\n",
    "ASPECT_WINDOW = 5                                 # no se usa aquí (dependencias), lo dejamos por claridad\n",
    "\n",
    "# Opcional: si quieres muestrear para pruebas rápidas (ej. 10000)\n",
    "SAMPLE_N = None  # ej. 10000 o None para todo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89bb112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs cargados: 200000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date     query  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ⬅️ Celda 3: Carga del dataset\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "if SAMPLE_N:\n",
    "    df = df.sample(SAMPLE_N, random_state=42).reset_index(drop=True)\n",
    "\n",
    "assert TEXT_COL in df.columns, f\"No encuentro la columna de texto '{TEXT_COL}'\"\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "ids = df[ID_COL].tolist() if (ID_COL and ID_COL in df.columns) else list(range(len(texts)))\n",
    "\n",
    "print(f\"Docs cargados: {len(texts)}\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a80ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo tokens: ['user', 'url', 'aww', \"that's\", 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Preprocesado y tokenización\n",
    "# Buscamos: lowercase, normalizar URLs/usuarios/hashtags, reducir elongaciones, tokenizar y quitar stopwords.\n",
    "\n",
    "URL_RE       = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "USER_RE      = re.compile(r\"@\\w+\")\n",
    "HASHTAG_RE   = re.compile(r\"#\")\n",
    "MULTICHAR_RE = re.compile(r\"(.)\\1{2,}\")  # \"buenoooo\" -> \"buenoo\"\n",
    "TOKEN_RE     = re.compile(r\"[a-záéíóúñü]+(?:'[a-záéíóúñü]+)?\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = URL_RE.sub(\" <url> \", s)\n",
    "    s = USER_RE.sub(\" <user> \", s)\n",
    "    s = HASHTAG_RE.sub(\"\", s)           # quita \"#\"\n",
    "    s = MULTICHAR_RE.sub(r\"\\1\\1\", s)    # limita repes a 2\n",
    "    # separa \"muyBueno\" -> \"muy bueno\" (simple)\n",
    "    s = re.sub(r\"([a-záéíóúñ])([A-ZÁÉÍÓÚÑ])\", r\"\\1 \\2\", s)\n",
    "    return s\n",
    "\n",
    "def tokenize(s: str, lang: str=\"en\"):\n",
    "    tokens = TOKEN_RE.findall(s)\n",
    "    sw_lang = \"spanish\" if lang == \"es\" else \"english\"\n",
    "    sw = set(stopwords.words(sw_lang))\n",
    "    # quitamos stopwords muy básicas; queremos quedarnos con señales de opinión\n",
    "    tokens = [t for t in tokens if t not in sw]\n",
    "    return tokens\n",
    "\n",
    "norm_texts = [normalize_text(t) for t in texts]\n",
    "token_docs = [tokenize(t, LANG) for t in norm_texts]\n",
    "\n",
    "# Frecuencias por si luego queremos filtrar WordNet por \"existe en corpus\"\n",
    "token_freq = Counter([tok for doc in token_docs for tok in doc])\n",
    "\n",
    "print(\"Ejemplo tokens:\", token_docs[0][:20] if token_docs else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b218ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño léxico: 190 (pos=121, neg=74)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fast', 1.0),\n",
       " ('splendid', 1.0),\n",
       " ('positive', 1.0),\n",
       " ('groovy', 1.0),\n",
       " ('dearest', 1.0),\n",
       " ('honey', 1.0),\n",
       " ('everlasting', 1.0),\n",
       " ('gravel', 1.0),\n",
       " ('screw', 1.0),\n",
       " ('break', 1.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Léxico base + expansión con WordNet\n",
    "# Mantenemos semillas cortas y claras. La expansión se filtra por \"aparece al menos 1 vez en corpus\".\n",
    "\n",
    "SEEDS_ES_POS = {\"bueno\",\"excelente\",\"fantastico\",\"fantástico\",\"util\",\"útil\",\"rapido\",\"rápido\",\"genial\",\"positivo\",\"satisfecho\",\"perfecto\",\"increible\",\"increíble\",\"mejor\",\"encanta\"}\n",
    "SEEDS_ES_NEG = {\"malo\",\"terrible\",\"pesimo\",\"pésimo\",\"inutil\",\"inútil\",\"lento\",\"horrible\",\"negativo\",\"insatisfecho\",\"defectuoso\",\"odio\",\"peor\",\"asco\"}\n",
    "SEEDS_EN_POS = {\"good\",\"great\",\"excellent\",\"fantastic\",\"useful\",\"fast\",\"awesome\",\"positive\",\"happy\",\"satisfied\",\"perfect\",\"amazing\",\"love\",\"better\"}\n",
    "SEEDS_EN_NEG = {\"bad\",\"terrible\",\"awful\",\"useless\",\"slow\",\"horrible\",\"negative\",\"unsatisfied\",\"defective\",\"hate\",\"worst\",\"poor\",\"worse\"}\n",
    "\n",
    "NEGATORS_ES = {\"no\",\"nunca\",\"jamas\",\"jamás\",\"sin\"}\n",
    "NEGATORS_EN = {\"no\",\"not\",\"never\",\"without\"}\n",
    "INTENSIFIERS_ES = {\"muy\":1.5,\"super\":1.5,\"súper\":1.5,\"re\":1.3,\"poco\":0.5}\n",
    "INTENSIFIERS_EN = {\"very\":1.5,\"super\":1.5,\"really\":1.3,\"so\":1.3,\"slightly\":0.7,\"little\":0.5}\n",
    "\n",
    "def expand_wordnet(words: set, lang: str, token_freq: Counter) -> set:\n",
    "    \"\"\"Expansión: sinónimos de WordNet; nos quedamos con unigrams que aparezcan al menos 1 vez.\"\"\"\n",
    "    out = set(words)\n",
    "    for w in list(words):\n",
    "        if lang == \"es\":\n",
    "            syns = wn.synsets(w, lang=\"spa\")\n",
    "            for s in syns:\n",
    "                for lemma in s.lemma_names(\"spa\"):\n",
    "                    lemma = lemma.replace(\"_\", \" \").lower()\n",
    "                    if \" \" in lemma: \n",
    "                        continue\n",
    "                    if token_freq.get(lemma, 0) < 1:\n",
    "                        continue\n",
    "                    out.add(lemma)\n",
    "        else:\n",
    "            syns = wn.synsets(w)\n",
    "            for s in syns:\n",
    "                for lemma in s.lemma_names():\n",
    "                    lemma = lemma.replace(\"_\", \" \").lower()\n",
    "                    if \" \" in lemma:\n",
    "                        continue\n",
    "                    if token_freq.get(lemma, 0) < 1:\n",
    "                        continue\n",
    "                    out.add(lemma)\n",
    "    return out\n",
    "\n",
    "if LANG == \"es\":\n",
    "    pos_set, neg_set = set(SEEDS_ES_POS), set(SEEDS_ES_NEG)\n",
    "    NEGATORS = NEGATORS_ES\n",
    "    INTENS   = INTENSIFIERS_ES\n",
    "else:\n",
    "    pos_set, neg_set = set(SEEDS_EN_POS), set(SEEDS_EN_NEG)\n",
    "    NEGATORS = NEGATORS_EN\n",
    "    INTENS   = INTENSIFIERS_EN\n",
    "\n",
    "if USE_WORDNET:\n",
    "    pos_set = expand_wordnet(pos_set, LANG, token_freq)\n",
    "    neg_set = expand_wordnet(neg_set, LANG, token_freq)\n",
    "\n",
    "lexicon = {w:  1.0 for w in pos_set}\n",
    "lexicon.update({w: -1.0 for w in neg_set})\n",
    "\n",
    "print(f\"Tamaño léxico: {len(lexicon)} (pos={len(pos_set)}, neg={len(neg_set)})\")\n",
    "list(lexicon.items())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229a913c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>label_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  score label_pred\n",
       "0   0    0.0        neg\n",
       "1   1    0.0        neg\n",
       "2   2    0.0        neg\n",
       "3   3    0.0        neg\n",
       "4   4    0.0        neg\n",
       "5   5    0.0        neg\n",
       "6   6    0.0        neg\n",
       "7   7    0.0        neg\n",
       "8   8    0.0        neg\n",
       "9   9    0.0        neg"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clasificador por reglas (score + etiquetas)\n",
    "# Los intensificadores multiplican el siguiente término polar; negación invierte 3 términos siguientes.\n",
    "\n",
    "def score_document(tokens, lexicon, negators, intensifiers):\n",
    "    score = 0.0\n",
    "    negate_left = 0\n",
    "    mult = 1.0\n",
    "    for tok in tokens:\n",
    "        if tok in negators:\n",
    "            negate_left = 3\n",
    "            continue\n",
    "        if tok in intensifiers:\n",
    "            mult = intensifiers[tok]\n",
    "            continue\n",
    "        if tok in lexicon:\n",
    "            s = lexicon[tok]\n",
    "            if negate_left > 0:\n",
    "                s = -s\n",
    "            s = s * mult\n",
    "            score += s\n",
    "            mult = 1.0\n",
    "            if negate_left > 0:\n",
    "                negate_left -= 1\n",
    "        else:\n",
    "            # si no aporta, reseteamos multiplicador y avanzamos negación si toca\n",
    "            mult = 1.0\n",
    "            if negate_left > 0:\n",
    "                negate_left -= 1\n",
    "    return score\n",
    "\n",
    "def label_from_scores(scores, tau_percentile=35):\n",
    "    abs_s = np.abs(scores)\n",
    "    tau = np.percentile(abs_s, tau_percentile) if len(abs_s) else 0.0\n",
    "    labels = []\n",
    "    for s in scores:\n",
    "        if abs(s) < tau:\n",
    "            labels.append(\"neu\")\n",
    "        elif s > 0:\n",
    "            labels.append(\"pos\")\n",
    "        else:\n",
    "            labels.append(\"neg\")\n",
    "    return labels, tau\n",
    "\n",
    "scores = np.array([score_document(doc, lexicon, NEGATORS, INTENS) for doc in token_docs], dtype=float)\n",
    "labels, tau = label_from_scores(scores, TAU_PERCENTILE)\n",
    "\n",
    "pred_df = pd.DataFrame({\"id\": ids, \"score\": scores, \"label_pred\": labels})\n",
    "pred_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe52213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docs': 200000,\n",
       " 'tau': 0.0,\n",
       " '%neu': np.float64(0.0),\n",
       " '%pos': np.float64(21.81),\n",
       " '%neg': np.float64(78.19),\n",
       " 'lexicon_size': 190}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resumen pequeño de resultados de sentimiento (sanity check)\n",
    "\n",
    "summary = {\n",
    "    \"docs\": len(pred_df),\n",
    "    \"tau\": float(tau),\n",
    "    \"%neu\": round((pred_df[\"label_pred\"]==\"neu\").mean()*100, 2),\n",
    "    \"%pos\": round((pred_df[\"label_pred\"]==\"pos\").mean()*100, 2),\n",
    "    \"%neg\": round((pred_df[\"label_pred\"]==\"neg\").mean()*100, 2),\n",
    "    \"lexicon_size\": len(lexicon)\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63bc0a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>day</td>\n",
       "      <td>854</td>\n",
       "      <td>619</td>\n",
       "      <td>1473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>time</td>\n",
       "      <td>480</td>\n",
       "      <td>360</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>thing</td>\n",
       "      <td>350</td>\n",
       "      <td>349</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>morning</td>\n",
       "      <td>530</td>\n",
       "      <td>49</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>night</td>\n",
       "      <td>365</td>\n",
       "      <td>170</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>friend</td>\n",
       "      <td>448</td>\n",
       "      <td>31</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>luck</td>\n",
       "      <td>312</td>\n",
       "      <td>53</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>weekend</td>\n",
       "      <td>204</td>\n",
       "      <td>91</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sleep</td>\n",
       "      <td>264</td>\n",
       "      <td>29</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>mother</td>\n",
       "      <td>272</td>\n",
       "      <td>20</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>weather</td>\n",
       "      <td>146</td>\n",
       "      <td>145</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>headache</td>\n",
       "      <td>116</td>\n",
       "      <td>175</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>news</td>\n",
       "      <td>102</td>\n",
       "      <td>153</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>idea</td>\n",
       "      <td>169</td>\n",
       "      <td>79</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>guy</td>\n",
       "      <td>95</td>\n",
       "      <td>152</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       aspect  pos_count  neg_count  total\n",
       "2         day        854        619   1473\n",
       "14       time        480        360    840\n",
       "46      thing        350        349    699\n",
       "62    morning        530         49    579\n",
       "84      night        365        170    535\n",
       "121    friend        448         31    479\n",
       "28       luck        312         53    365\n",
       "347   weekend        204         91    295\n",
       "25      sleep        264         29    293\n",
       "344    mother        272         20    292\n",
       "303   weather        146        145    291\n",
       "90   headache        116        175    291\n",
       "41       news        102        153    255\n",
       "38       idea        169         79    248\n",
       "23        guy         95        152    247"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aspectos por dependecias\n",
    "#   !python -m spacy download es_core_news_sm  (para español)\n",
    "\n",
    "import spacy\n",
    "\n",
    "model_name = \"es_core_news_sm\" if LANG == \"es\" else \"en_core_web_sm\"\n",
    "try:\n",
    "    nlp = spacy.load(model_name)\n",
    "except Exception as e:\n",
    "    # Intento de descarga automática (requiere internet). Si falla, te digo cómo instalar.\n",
    "    try:\n",
    "        from spacy.cli import download\n",
    "        download(model_name)\n",
    "        nlp = spacy.load(model_name)\n",
    "    except Exception as e2:\n",
    "        nlp = None\n",
    "        print(f\"No pude cargar '{model_name}'. Instala manualmente con: python -m spacy download {model_name}\")\n",
    "\n",
    "def extract_aspects_dep(texts, nlp, lexicon):\n",
    "    \"\"\"\n",
    "    Reglas MUY sencillas:\n",
    "      - amod: NOUN <- ADJ    -> aspecto=NOUN, opinión=ADJ\n",
    "      - copular: ADJ con nsubj NOUN\n",
    "      - obj: VERB(opinión) -> obj/dobj NOUN\n",
    "    Sumamos +1/-1 al aspecto según polaridad en el léxico.\n",
    "    \"\"\"\n",
    "    if nlp is None:\n",
    "        # si no hay modelo, devolvemos vacío para no romper el notebook\n",
    "        return pd.DataFrame(columns=[\"aspect\",\"pos_count\",\"neg_count\",\"total\"])\n",
    "\n",
    "    counts = defaultdict(lambda: {\"pos\":0,\"neg\":0,\"total\":0})\n",
    "    for doc in nlp.pipe(texts, disable=[\"ner\"]):\n",
    "        # 1) amod\n",
    "        for tok in doc:\n",
    "            if tok.dep_ == \"amod\" and tok.pos_ == \"ADJ\" and tok.head.pos_ == \"NOUN\":\n",
    "                aspect = tok.head.lemma_.lower()\n",
    "                opinion = tok.lemma_.lower()\n",
    "                pol = lexicon.get(opinion, 0)\n",
    "                if pol != 0:\n",
    "                    if pol > 0: counts[aspect][\"pos\"] += 1\n",
    "                    else:        counts[aspect][\"neg\"] += 1\n",
    "                    counts[aspect][\"total\"] += 1\n",
    "\n",
    "        # 2) copular (ADJ con sujeto nominal)\n",
    "        for tok in doc:\n",
    "            if tok.pos_ == \"ADJ\":\n",
    "                subs = [c for c in tok.children if c.dep_.startswith(\"nsubj\") and c.pos_ == \"NOUN\"]\n",
    "                if subs:\n",
    "                    aspect = subs[0].lemma_.lower()\n",
    "                    opinion = tok.lemma_.lower()\n",
    "                    pol = lexicon.get(opinion, 0)\n",
    "                    if pol != 0:\n",
    "                        if pol > 0: counts[aspect][\"pos\"] += 1\n",
    "                        else:        counts[aspect][\"neg\"] += 1\n",
    "                        counts[aspect][\"total\"] += 1\n",
    "\n",
    "        # 3) verbo de opinión + objeto\n",
    "        for tok in doc:\n",
    "            if tok.pos_ == \"VERB\":\n",
    "                opinion = tok.lemma_.lower()\n",
    "                pol = lexicon.get(opinion, 0)\n",
    "                if pol == 0:\n",
    "                    continue\n",
    "                objs = [c for c in tok.children if c.dep_ in (\"obj\",\"dobj\") and c.pos_ in (\"NOUN\",\"PROPN\")]\n",
    "                for obj in objs:\n",
    "                    aspect = obj.lemma_.lower()\n",
    "                    if pol > 0: counts[aspect][\"pos\"] += 1\n",
    "                    else:        counts[aspect][\"neg\"] += 1\n",
    "                    counts[aspect][\"total\"] += 1\n",
    "\n",
    "    rows = [{\"aspect\":a, \"pos_count\":d[\"pos\"], \"neg_count\":d[\"neg\"], \"total\":d[\"total\"]} \n",
    "            for a,d in counts.items()]\n",
    "    df_aspects = pd.DataFrame(rows).sort_values([\"total\",\"pos_count\"], ascending=[False, False])\n",
    "    return df_aspects\n",
    "\n",
    "aspects_df = extract_aspects_dep(norm_texts, nlp, lexicon)\n",
    "aspects_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f854ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
